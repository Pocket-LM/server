import asyncio
import json
from datasets import Dataset, load_dataset
from httpx import AsyncClient
from langchain.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from ragas import evaluate
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.metrics import (
    ResponseRelevancy,
    Faithfulness,
    LLMContextPrecisionWithReference,
    LLMContextRecall,
)
from pydantic import SecretStr

from src.configs.settings import settings
from src.schemas.langgraph import AgentState
from src.utils.logging import get_logger
from src.utils.langgraph.agent import get_checkpoint_saver, get_langgraph_agent

"""
  The used hugging face dataset contains the following fields:

    user_input: The question to be answered
    reference: Ground truth answer for evaluation (much better than response)
    response: Generated answer from the system
    retrieved_contexts: List of relevant context passages retrieved for answering the question

  Context Pression:
    Evaluates whether retrieved contexts (generated by RAG retrieval) are useful for answering a question by comparing each context against a reference answer (ground truth answer for evaluation).
    Used => user_input, reference, retrieved_contexts
  
  Context Recall:
    Evaluates whether retrieved contexts are useful by comparing each context against the generated response. Use this when you don't have a reference answer but have the response that was generated.
    Used => retrieved_contexts and reference_contexts

  Response Relevancy:
    Used => user_input and response

  Faithfulness:
    Used => response and retrieved_contexts
"""

logger = get_logger(__name__)
ragas_llm = LangchainLLMWrapper(
    langchain_llm=ChatGoogleGenerativeAI(  # type: ignore
        google_api_key=settings.GEMINI_API_KEY, model=settings.GEMINI_LLM_MODEL
    )
)

ragas_embedding = LangchainEmbeddingsWrapper(
    embeddings=GoogleGenerativeAIEmbeddings(  # type: ignore
        google_api_key=SecretStr(settings.GEMINI_API_KEY),
        model=settings.GEMINI_EMBEDDING_MODEL,
    ),
)


async def embed_retrieved_contexts():
    hf_dataset = load_dataset("explodinggradients/amnesty_qa", "english_v3")
    async with AsyncClient(base_url="http://localhost:8000/api/v1") as client:
        for idx, dt in enumerate(hf_dataset["eval"]):
            try:
                await client.post(
                    url="/capture",
                    data={
                        "type": "selection",
                        "knowledgeBase": "default",
                        "selection": dt["retrieved_contexts"][0],
                        "url": "http://example.com/document",
                    },
                )
            except Exception as e:
                logger.info(f"Error embedding context for index {idx}: {e}")
                return False

    logger.info("Successfully embedded all retrieved contexts.")
    return True


async def generate_single_turn_dts():
    hf_dataset = load_dataset("explodinggradients/amnesty_qa", "english_v3")
    generated_dataset: dict = {
        "user_input": [],
        "reference": [],
        "response": [],
        "retrieved_contexts": [],
        "reference_contexts": [],
    }

    for sample in hf_dataset["eval"]:
        async with get_langgraph_agent(
            user_id=settings.DEFAULT_USER_ID, thread_id=settings.DEFAULT_SESSION_ID
        ) as (
            graph,
            config,
        ):
            # To only keep a single turn interaction
            async with get_checkpoint_saver() as checkpoint_saver:
                if await checkpoint_saver.aget(config=config):
                    await checkpoint_saver.adelete_thread(
                        thread_id=settings.DEFAULT_SESSION_ID
                    )

            human_msg = HumanMessage(content=sample["user_input"])  # type: ignore

            try:
                state = await graph.ainvoke(
                    AgentState(
                        messages=[human_msg],
                        summary="",
                        history=[human_msg],
                    ),
                    config,
                    stream_mode="values",
                )

                retrieved_contexts: list[str] = []
                for msg in reversed(state.get("messages", [])):
                    if msg.type == "tool":
                        retrieved_contexts.extend(msg.content.split("\n\n\n\n"))

                generated_dataset["user_input"].append(sample["user_input"])  # type: ignore
                generated_dataset["reference"].append(sample["reference"])  # type: ignore
                generated_dataset["response"].append(
                    state.get("messages", [])[-1].content
                )
                generated_dataset["retrieved_contexts"].append(retrieved_contexts)
                generated_dataset["reference_contexts"].append(sample["retrieved_contexts"])  # type: ignore

                # To avoid rate limiting issues
                await asyncio.sleep(60)

            except Exception as e:
                logger.info(
                    f"Error processing message for input: {sample['user_input']}: {e}"  # type: ignore
                )
                return False

    with open("test/data/single_turn_dts.json", "w") as f:
        json.dump(generated_dataset, f, indent=2)

    return True


async def generate_multi_turn_dts():
    hf_dataset = load_dataset("explodinggradients/amnesty_qa", "english_v3")
    generated_dataset: dict = {
        "user_input": [],
        "reference": [],
        "response": [],
        "retrieved_contexts": [],
        "reference_contexts": [],
    }

    checkpoint_cleaned = False
    for sample in hf_dataset["eval"]:
        async with get_langgraph_agent(
            user_id=settings.DEFAULT_USER_ID, thread_id=settings.DEFAULT_SESSION_ID
        ) as (
            graph,
            config,
        ):
            # Clean the checkpoint only once at the start
            if not checkpoint_cleaned:
                async with get_checkpoint_saver() as checkpoint_saver:
                    if await checkpoint_saver.aget(config=config):
                        await checkpoint_saver.adelete_thread(
                            thread_id=settings.DEFAULT_SESSION_ID
                        )
                checkpoint_cleaned = True

            # No more removing the previous conversation to allow multi-turn interactions
            human_msg = HumanMessage(content=sample["user_input"])  # type: ignore

            try:
                state = await graph.ainvoke(
                    AgentState(
                        messages=[human_msg],
                        summary="",
                        history=[human_msg],
                    ),
                    config,
                    stream_mode="values",
                )

                retrieved_contexts: list[str] = []
                for msg in reversed(state.get("messages", [])):
                    if msg.type == "tool":
                        retrieved_contexts.extend(msg.content.split("\n\n\n\n"))
                        break  # Only take the latest retrieval

                generated_dataset["user_input"].append(sample["user_input"])  # type: ignore
                generated_dataset["reference"].append(sample["reference"])  # type: ignore
                generated_dataset["response"].append(
                    state.get("messages", [])[-1].content
                )
                generated_dataset["retrieved_contexts"].append(retrieved_contexts)
                generated_dataset["reference_contexts"].append(
                    sample["retrieved_contexts"]  # type: ignore
                )

                # To avoid rate limiting issues
                await asyncio.sleep(60)

            except Exception as e:
                logger.info(
                    f"Error processing message for input: {sample['user_input']}: {e}"  # type: ignore
                )
                return False

    with open("test/data/multi_turn_dts.json", "w") as f:
        json.dump(generated_dataset, f, indent=2)

    return True


async def eval_single_turn(name: str = ""):
    with open(
        f"test/data/{name + "_" if len(name) > 0 else ""}single_turn_dts.json", "r"
    ) as f:
        generated = json.load(f)

    result = evaluate(
        dataset=Dataset.from_dict(generated),
        metrics=[
            LLMContextPrecisionWithReference(
                name="context_precision",
                llm=ragas_llm,
            ),
            LLMContextRecall(
                name="context_recall",
                llm=ragas_llm,
            ),
            ResponseRelevancy(
                name="response_relevancy",
                embeddings=ragas_embedding,
                llm=ragas_llm,
            ),
            Faithfulness(
                name="faithfulness",
                llm=ragas_llm,
            ),
        ],
    )

    result.to_pandas().to_json(
        f"test/result/{name + "_" if len(name) > 0 else ""}single_turn_eval_results.json",
        indent=2,
    )


async def eval_multi_turn(name: str = ""):
    with open(
        f"test/data/{name + "_" if len(name) > 0 else ""}multi_turn_dts.json", "r"
    ) as f:
        generated = json.load(f)

    result = evaluate(
        dataset=Dataset.from_dict(generated),
        metrics=[
            LLMContextPrecisionWithReference(
                name="context_precision",
                llm=ragas_llm,
            ),
            LLMContextRecall(
                name="context_recall",
                llm=ragas_llm,
            ),
            ResponseRelevancy(
                name="response_relevancy",
                embeddings=ragas_embedding,
                llm=ragas_llm,
            ),
            Faithfulness(
                name="faithfulness",
                llm=ragas_llm,
            ),
        ],
    )

    result.to_pandas().to_json(
        f"test/result/{name + "_" if len(name) > 0 else ""}multi_turn_eval_results.json",
        indent=2,
    )


async def main():
    # await embed_retrieved_contexts()

    # if not (await generate_single_turn_dts()):
    #     logger.info("Failed to generate single turn dataset.")
    #     return

    # if not (await generate_multi_turn_dts()):
    #     logger.info("Failed to generate multi turn dataset.")
    #     return

    # await eval_single_turn(name="notebooklm")

    # await eval_multi_turn(name="notebooklm")

    return


if __name__ == "__main__":
    asyncio.run(main())
